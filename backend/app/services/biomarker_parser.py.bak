"""
Biomarker Parser Service

This module provides services for parsing biomarker data from lab reports using
Claude API and standardizing the extracted data using the biomarker dictionary.
"""
import json
import re
import logging
import os
import threading
import time
from typing import Dict, List, Any, Optional, Tuple, Union
import httpx
from datetime import datetime, timedelta
import random

from app.services.biomarker_dictionary import (
    get_standardized_biomarker_name,
    convert_to_standard_unit,
    get_biomarker_category,
    get_reference_range,
    BIOMARKER_DICT
)

# Configure logging with more detailed format
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Set up a file handler to also log to a file
log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'logs')
os.makedirs(log_dir, exist_ok=True)
file_handler = logging.FileHandler(os.path.join(log_dir, 'biomarker_parser.log'))
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'))
logger.addHandler(file_handler)

# Set debug level to enable all logging messages
logger.setLevel(logging.DEBUG)

# Load Claude API key from environment variable
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
CLAUDE_API_URL = "https://api.anthropic.com/v1/messages"

# Set this to True to save all Claude API requests and responses for debugging
SAVE_CLAUDE_RESPONSES = True

# Configure extraction parameters
MAX_TEXT_LENGTH = 25000  # Maximum text length to send to Claude in one request
MAX_API_TIMEOUT = 120    # Increased timeout from 60 to 120 seconds
MAX_RETRIES = 2          # Number of retries for API calls

# Dictionary of common biomarker aliases for robust matching
BIOMARKER_ALIASES = {
    "glucose": ["blood glucose", "fasting glucose", "plasma glucose", "gluc", "glu"],
    "hemoglobin a1c": ["hba1c", "a1c", "glycated hemoglobin", "glycosylated hemoglobin", "hemoglobin a1c"],
    "total cholesterol": ["cholesterol", "tc", "total chol", "chol, total"],
    "hdl cholesterol": ["hdl", "hdl-c", "high density lipoprotein", "good cholesterol"],
    "ldl cholesterol": ["ldl", "ldl-c", "low density lipoprotein", "bad cholesterol"],
    "triglycerides": ["tg", "trigs", "triglyceride"],
    "tsh": ["thyroid stimulating hormone", "thyrotropin", "thyroid function"],
    "free t4": ["ft4", "thyroxine", "free thyroxine"],
    "free t3": ["ft3", "triiodothyronine", "free triiodothyronine"],
    "vitamin d": ["25-hydroxyvitamin d", "25-oh vitamin d", "vitamin d, 25-hydroxy", "vit d"],
    "vitamin b12": ["b12", "cobalamin", "vit b12"],
    "ferritin": ["ferr", "serum ferritin"],
    "iron": ["fe", "serum iron"],
    "transferrin": ["tf", "trf"],
    "tibc": ["total iron binding capacity"],
    "creatinine": ["creat", "cr", "serum creatinine"],
    "bun": ["blood urea nitrogen", "urea nitrogen"],
    "egfr": ["estimated glomerular filtration rate", "gfr"],
    "alt": ["alanine aminotransferase", "sgpt"],
    "ast": ["aspartate aminotransferase", "sgot"],
    "alkaline phosphatase": ["alp", "alk phos"],
    "total bilirubin": ["tbili", "bilirubin total"],
    "albumin": ["alb", "serum albumin"],
    "total protein": ["tp", "protein, total"],
    "sodium": ["na", "na+", "serum sodium"],
    "potassium": ["k", "k+", "serum potassium"],
    "chloride": ["cl", "cl-", "serum chloride"],
    "bicarbonate": ["hco3", "hco3-", "co2", "carbon dioxide"],
    "calcium": ["ca", "ca2+", "serum calcium"],
    "magnesium": ["mg", "mg2+", "serum magnesium"],
    "phosphorus": ["p", "phos", "phosphate", "serum phosphorus"],
    "uric acid": ["ua", "serum uric acid"],
    "hemoglobin": ["hgb", "hb", "hg"],
    "hematocrit": ["hct", "ht"],
    "wbc": ["white blood cell count", "white blood cells", "leukocytes"],
    "platelet count": ["plt", "platelets"],
    "mch": ["mean corpuscular hemoglobin"],
    "mchc": ["mean corpuscular hemoglobin concentration"],
    "mcv": ["mean corpuscular volume"],
    "rdw": ["red cell distribution width"],
    "neutrophils": ["neut", "neutrophil count", "polys"],
    "lymphocytes": ["lymphs", "lymphocyte count"],
    "monocytes": ["mono", "monocyte count"],
    "eosinophils": ["eos", "eosinophil count"],
    "basophils": ["baso", "basophil count"],
    "psa": ["prostate specific antigen"],
    "c-reactive protein": ["crp", "c reactive protein"],
    "esr": ["erythrocyte sedimentation rate", "sed rate"],
    "homocysteine": ["hcy"],
    "cortisol": ["cort", "serum cortisol"],
    "testosterone": ["test", "total testosterone"],
    "estradiol": ["e2"],
    "progesterone": ["prog"],
    "dhea-s": ["dehydroepiandrosterone sulfate"],
    "folate": ["folic acid"],
    "hla-b27": ["human leukocyte antigen b27"],
    "ana": ["antinuclear antibody"],
    "rf": ["rheumatoid factor"],
    "tpo antibodies": ["thyroid peroxidase antibodies", "anti-tpo", "thyroid antibodies"],
}

class TimeoutError(Exception):
    """Exception raised when a function call times out"""
    pass

def with_timeout(timeout_seconds, default_return=None):
    """Decorator to apply a timeout to a function"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            result = [default_return]
            exception = [None]
            
            def target():
                try:
                    result[0] = func(*args, **kwargs)
                except Exception as e:
                    exception[0] = e
            
            thread = threading.Thread(target=target)
            thread.daemon = True
            thread.start()
            thread.join(timeout_seconds)
            
            if thread.is_alive():
                logger.warning(f"[TIMEOUT] Function {func.__name__} timed out after {timeout_seconds} seconds")
                return default_return
            
            if exception[0]:
                logger.error(f"[FUNCTION_ERROR] Function {func.__name__} raised: {str(exception[0])}")
                raise exception[0]
                
            return result[0]
        return wrapper
    return decorator

def chunk_text(text: str, max_chunk_size: int = MAX_TEXT_LENGTH) -> List[str]:
    """
    Split text into smaller chunks if it exceeds max_chunk_size.
    Try to split at paragraph boundaries when possible.
    
    Args:
        text: The text to chunk
        max_chunk_size: Maximum size of each chunk
        
    Returns:
        List of text chunks
    """
    # If text is already small enough, return it as is
    if len(text) <= max_chunk_size:
        return [text]
    
    chunks = []
    current_pos = 0
    
    while current_pos < len(text):
        # If remaining text fits in one chunk, add it and break
        if current_pos + max_chunk_size >= len(text):
            chunks.append(text[current_pos:])
            break
            
        # Find a good break point - prefer paragraph breaks
        end_pos = current_pos + max_chunk_size
        
        # Look for paragraph breaks near the end of the chunk
        paragraph_break = text.rfind('\n\n', current_pos, end_pos)
        if paragraph_break != -1 and paragraph_break > current_pos + max_chunk_size * 0.75:
            # Found a paragraph break that's at least 75% into the chunk
            end_pos = paragraph_break
        else:
            # Try to find a line break
            line_break = text.rfind('\n', current_pos, end_pos)
            if line_break != -1 and line_break > current_pos + max_chunk_size * 0.85:
                end_pos = line_break
            else:
                # Try to find a sentence break
                sentence_break = text.rfind('. ', current_pos, end_pos)
                if sentence_break != -1 and sentence_break > current_pos + max_chunk_size * 0.9:
                    end_pos = sentence_break + 1  # Include the period
        
        chunks.append(text[current_pos:end_pos])
        current_pos = end_pos
        
    logger.info(f"[TEXT_CHUNKING] Split text of length {len(text)} into {len(chunks)} chunks")
    return chunks

def extract_biomarkers_with_claude(text: str, filename: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Extract biomarkers from PDF text using Claude API.
    
    Args:
        text: Text content from PDF
        filename: Name of the file for logging
        
    Returns:
        Tuple containing a list of biomarkers and metadata
    """
    logger.info(f"[CLAUDE_EXTRACTION_START] Extracting biomarkers from {filename}")
    start_time = datetime.now()
    
    # Preprocess the text
    processed_text = _preprocess_text_for_claude(text)
    logger.debug(f"[TEXT_PREPROCESSING] Preprocessed text from {len(text)} to {len(processed_text)} characters")
    
    # Check if we need to chunk the text
    if len(processed_text) > MAX_TEXT_LENGTH:
        return _process_large_text(processed_text, filename)
    
    # Prepare the prompt for Claude
    prompt = f"""
You are a medical laboratory data extraction specialist. Your task is to extract biomarker information from the medical lab report text below, focusing ONLY on legitimate clinical biomarkers that have a measurable result.

IMPORTANT: Biomarkers have these key characteristics:
1. They have a measurable numerical result (e.g., 95, 5.2, <0.5)
2. They have a unit of measurement (e.g., mg/dL, mmol/L)
3. They typically have a reference range (e.g., 70-99, <5.0)
4. They are specific tests measuring something in the body (glucose, cholesterol, vitamin levels, etc.)

For EACH biomarker you identify, provide:
- name: The standardized name of the biomarker
- original_name: The name exactly as it appears in the report
- value: The numerical value of the result (as a number, convert ranges to the midpoint)
- original_value: The original result text as seen in the report
- unit: The standardized unit of measurement
- original_unit: The unit exactly as it appears in the report
- reference_range: The reference/normal range as text
- reference_range_low: The lower bound of the reference range as a number (if available)
- reference_range_high: The upper bound of the reference range as a number (if available)
- category: Categorize into: Lipid, Metabolic, Liver, Kidney, Electrolyte, Blood, Thyroid, Vitamin, Hormone, Immunology, Cardiovascular, or Other
- is_abnormal: true if the result is outside the reference range, false otherwise
- confidence: A number between 0.0 and 1.0 representing your confidence that this is a legitimate biomarker

DO NOT extract:
- Page numbers, headers, footers, or section titles
- Patient information like names, dates of birth, addresses
- Formatting elements like columns, rows, or delimiters
- Test or sample IDs
- Collection times or dates
- Reference ranges by themselves without corresponding measurements
- Text that looks like URLs, email addresses, or identifiers
- Sentences or paragraphs of explanatory text

Provide all results in this exact JSON format:
{{
  "biomarkers": [
    {{
      "name": "Glucose",
      "original_name": "Glucose, Fasting",
      "value": 95,
      "original_value": "95",
      "unit": "mg/dL",
      "original_unit": "mg/dL",
      "reference_range": "70-99 mg/dL",
      "reference_range_low": 70,
      "reference_range_high": 99,
      "category": "Metabolic",
      "is_abnormal": false,
      "confidence": 0.98
    }},
    ...
  ],
  "metadata": {{
    "lab_name": "LabCorp",
    "report_date": "2022-04-15",
    "provider": "Dr. Smith",
    "patient_name": "REDACTED"
  }}
}}

Here is the lab report text to extract from:

{processed_text}
"""

    try:
        logger.debug("[CLAUDE_API_CALL] Sending request to Claude API")
        api_start_time = datetime.now()
        
        # Get the configured Claude API key
        import os
        api_key = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY")
        if not api_key:
            logger.error("[API_KEY_ERROR] Claude API key not found in environment variables")
            raise ValueError("Claude API key not found. Set ANTHROPIC_API_KEY environment variable.")
        
        import anthropic
        client = anthropic.Anthropic(api_key=api_key)
        
        # Add retry mechanism with backoff
        for attempt in range(MAX_RETRIES + 1):
            try:
                # Use the timeout wrapper for the API call
                @with_timeout(timeout_seconds=MAX_API_TIMEOUT, default_return=None)  # Increased from 60 to 120 seconds
                def call_claude_api():
                    # Make the API call with max tokens and timeout
                    # Use Sonnet model which has better tradeoff between accuracy and speed compared to Opus
                    response = client.messages.create(
                        model="claude-3-sonnet-20240229",  # Changed from opus to sonnet for better performance
                        max_tokens=4000,
                        temperature=0.1,
                        system="You are a biomarker extraction expert specializing in parsing medical lab reports. Extract ONLY valid clinical biomarkers with measurements and reference ranges. Avoid patient info, headers, footers, and page numbers.",
                        messages=[
                            {"role": "user", "content": prompt}
                        ]
                    )
                    return response
                
                # Call the API with timeout
                response = call_claude_api()
                
                # If the call timed out but we have more retries left
                if response is None and attempt < MAX_RETRIES:
                    # Calculate backoff time (exponential with jitter)
                    backoff_time = min(30, (2 ** attempt) + random.uniform(0, 1))
                    logger.warning(f"[CLAUDE_API_RETRY] Attempt {attempt+1}/{MAX_RETRIES+1} timed out. Retrying in {backoff_time:.2f} seconds...")
                    time.sleep(backoff_time)
                    continue
                
                # If the call succeeded or we're out of retries, break the loop
                break
                
            except Exception as api_error:
                # Handle API errors with retries
                if attempt < MAX_RETRIES:
                    backoff_time = min(30, (2 ** attempt) + random.uniform(0, 1))
                    logger.warning(f"[CLAUDE_API_ERROR] Attempt {attempt+1}/{MAX_RETRIES+1} failed: {str(api_error)}. Retrying in {backoff_time:.2f} seconds...")
                    time.sleep(backoff_time)
                else:
                    logger.error(f"[CLAUDE_API_FAILED] All {MAX_RETRIES+1} attempts failed: {str(api_error)}")
                    raise
        
        # If all retries failed and the call timed out, fall back to text-based parser
        if response is None:
            logger.error("[CLAUDE_API_TIMEOUT] API call timed out after all retries")
            logger.info("[FALLBACK_TO_TEXT_PARSER] Using fallback parser due to timeout")
            fallback_results = parse_biomarkers_from_text(text)
            logger.info(f"[FALLBACK_PARSER] Found {len(fallback_results)} biomarkers")
            return fallback_results, {}
        
        api_duration = (datetime.now() - api_start_time).total_seconds()
        logger.info(f"[CLAUDE_API_RESPONSE] Received response from Claude API in {api_duration:.2f} seconds")
        
        # Get the response content
        response_content = response.content[0].text
        
        # Try to parse the JSON response
        try:
            # Extract the JSON part from the response using regex
            import re
            json_match = re.search(r'({[\s\S]*})', response_content)
            
            if json_match:
                json_str = json_match.group(1)
                logger.debug(f"[JSON_EXTRACTION] Extracted JSON string from Claude response: {len(json_str)} characters")
                
                # Parse the JSON
                parsed_response = json.loads(json_str)
                
                biomarkers = parsed_response.get("biomarkers", [])
                metadata = parsed_response.get("metadata", {})
                
                logger.info(f"[BIOMARKERS_EXTRACTED] Extracted {len(biomarkers)} biomarkers from Claude API response")
                
                # Log metadata if available
                if metadata:
                    logger.info(f"[METADATA_EXTRACTED] Extracted metadata: {json.dumps(metadata)}")
                
                # If no biomarkers were found, fall back to text-based parser
                if not biomarkers:
                    logger.warning("[NO_BIOMARKERS_FOUND] No biomarkers found in Claude API response")
                    logger.info("[FALLBACK_TO_TEXT_PARSER] Using fallback parser due to empty biomarkers list")
                    fallback_results = parse_biomarkers_from_text(text)
                    logger.info(f"[FALLBACK_PARSER] Found {len(fallback_results)} biomarkers")
                    return fallback_results, metadata
                
                # Filter biomarkers based on confidence score - require at least 60% confidence
                filtered_biomarkers = []
                for biomarker in biomarkers:
                    name = biomarker.get("name", "").strip()
                    value = biomarker.get("value", 0)
                    unit = biomarker.get("unit", "")
                    confidence = float(biomarker.get("confidence", 0.0))
                    
                    if confidence < 0.6:  # Only apply minimal confidence check
                        logger.warning(f"[LOW_CONFIDENCE_BIOMARKER] Skipping low confidence biomarker: {name} (confidence: {confidence})")
                        continue
                    
                    filtered_biomarkers.append(biomarker)
                
                logger.info(f"[FILTERED_BIOMARKERS] Filtered out {len(biomarkers) - len(filtered_biomarkers)} biomarkers based on confidence score")
                
                # Process the biomarkers to standardize format
                processing_start_time = datetime.now()
                processed_biomarkers = []
                for i, biomarker in enumerate(filtered_biomarkers):
                    try:
                        # Process and standardize the biomarker data
                        processed_biomarker = _process_biomarker(biomarker)
                        processed_biomarkers.append(processed_biomarker)
                    except Exception as e:
                        logger.error(f"[BIOMARKER_PROCESSING_ERROR] Error processing biomarker {i}: {str(e)}")
                        logger.error(f"[BIOMARKER_DATA] Problem biomarker data: {json.dumps(biomarker)}")
                
                processing_duration = (datetime.now() - processing_start_time).total_seconds()
                logger.debug(f"[BIOMARKER_PROCESSING] Processing {len(filtered_biomarkers)} biomarkers took {processing_duration:.2f} seconds")
                
                total_duration = (datetime.now() - start_time).total_seconds()
                logger.info(f"[BIOMARKER_EXTRACTION_COMPLETE] Total extraction took {total_duration:.2f} seconds")
                
                # If we have no valid biomarkers, try the fallback parser
                if not processed_biomarkers:
                    logger.warning("[NO_VALID_BIOMARKERS] No valid biomarkers found in Claude response. Using fallback parser.")
                    fallback_results = parse_biomarkers_from_text(text)
                    logger.info(f"[FALLBACK_PARSER] Found {len(fallback_results)} biomarkers")
                    return fallback_results, {}
                
                return processed_biomarkers, metadata
            else:
                logger.error("[JSON_PARSING_ERROR] Could not extract JSON from Claude API response")
                raise ValueError("Failed to extract JSON from Claude API response")
        except json.JSONDecodeError as json_error:
            logger.error(f"[JSON_PARSING_ERROR] Could not parse Claude API response as JSON: {str(json_error)}")
            logger.debug(f"[CLAUDE_RESPONSE] Raw response: {response_content[:500]}...")
            
            # Fall back to text-based parser
            logger.info("[FALLBACK_TO_TEXT_PARSER] Using fallback parser due to JSON parsing error")
            fallback_results = parse_biomarkers_from_text(text)
            logger.info(f"[FALLBACK_PARSER] Found {len(fallback_results)} biomarkers")
            return fallback_results, {}
    except Exception as e:
        logger.error(f"[CLAUDE_API_ERROR] Error calling Claude API: {str(e)}")
        
        # Fall back to text-based parser
        logger.info("[FALLBACK_TO_TEXT_PARSER] Using fallback parser due to Claude API error")
        fallback_results = parse_biomarkers_from_text(text)
        logger.info(f"[FALLBACK_PARSER] Found {len(fallback_results)} biomarkers")
        return fallback_results, {}

def validate_biomarker_with_claude(biomarker: Dict[str, Any]) -> float:
    """
    Use Claude to validate a biomarker with uncertain legitimacy.
    
    Args:
        biomarker: The biomarker data to validate
        
    Returns:
        confidence score between 0.0 and 1.0
    """
    try:
        # Extract key biomarker details for validation
        name = biomarker.get("name", "")
        value = biomarker.get("value", "")
        unit = biomarker.get("unit", "")
        reference_range = biomarker.get("reference_range", "")
        
        logger.info(f"[BIOMARKER_VALIDATION] Validating biomarker: {name} = {value} {unit}")
        
        # Construct a prompt for Claude to evaluate
        prompt = f"""
Is the following a legitimate clinical biomarker? 

Name: {name}
Value: {value}
Unit: {unit}
Reference Range: {reference_range}

A legitimate biomarker should:
1. Have a recognizable name for a substance or parameter measured in medical testing
2. Have an appropriate unit of measurement for that substance
3. Have a plausible value for a human biomarker
4. Have a reference range that makes sense for the biomarker

Please evaluate if this is a real biomarker and respond with a confidence score between 0.0 and 1.0, where:
- 0.0 means definitely NOT a legitimate biomarker
- 1.0 means definitely a legitimate biomarker
- Values in between represent your degree of confidence

Respond with ONLY the numeric confidence score.
"""
        
        # Get the configured Claude API key
        import os
        import anthropic
        
        api_key = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY")
        if not api_key:
            logger.error("[API_KEY_ERROR] Claude API key not found in environment variables")
            return 0.5  # Neutral confidence if we can't validate
        
        client = anthropic.Anthropic(api_key=api_key)
        
        # Use the timeout wrapper for the API call with retry logic
        response = None
        
        for attempt in range(MAX_RETRIES + 1):
            # Define the API call function with timeout
            @with_timeout(timeout_seconds=60, default_return=None)  # 60 second timeout for validation
            def call_claude_api():
                # Make a lightweight API call just for validation
                response = client.messages.create(
                    model="claude-3-haiku-20240307",  # Use a smaller, faster model for validation
                    max_tokens=10,
                    temperature=0.0,
                    system="You are evaluating whether something is a legitimate clinical biomarker.",
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                return response
            
            # Call the API with timeout
            response = call_claude_api()
            
            # If successful, break the loop
            if response is not None:
                break
                
            # If not successful but we have retries left
            if attempt < MAX_RETRIES:
                backoff_time = min(10, (2 ** attempt) + random.uniform(0, 1))
                logger.warning(f"[VALIDATION_API_RETRY] Attempt {attempt+1}/{MAX_RETRIES+1} timed out. Retrying in {backoff_time:.2f} seconds...")
                time.sleep(backoff_time)
        
        # If all attempts failed, return neutral confidence
        if response is None:
            logger.error("[CLAUDE_API_TIMEOUT_VALIDATION] API call timed out after all retries")
            return 0.5  # Neutral confidence if the API times out
        
        # Get the response content
        response_content = response.content[0].text.strip()
        
        # Try to extract a confidence score
        try:
            # Clean up the response to extract just the number
            clean_response = response_content.replace("Confidence score:", "").strip()
            confidence = float(clean_response)
            
            # Ensure the confidence is between 0 and 1
            confidence = max(0.0, min(1.0, confidence))
            
            logger.info(f"[VALIDATION_RESULT] Biomarker '{name}' validation confidence: {confidence}")
            return confidence
        except ValueError:
            logger.warning(f"[VALIDATION_PARSING_ERROR] Could not parse validation response: {response_content}")
            return 0.5  # Default to neutral confidence
    except Exception as e:
        logger.error(f"[VALIDATION_ERROR] Error validating biomarker: {str(e)}")
        return 0.5  # Default to neutral confidence on error

def _process_biomarker(biomarker: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process and standardize a biomarker extracted from Claude API.
    
    Args:
        biomarker: Dictionary containing biomarker data
        
    Returns:
        Processed and standardized biomarker
    """
    # Start with the original biomarker data
    result = {**biomarker}
    
    # Cache for standardized names to avoid repeated lookups
    if not hasattr(_process_biomarker, "_name_cache"):
        _process_biomarker._name_cache = {}
    
    try:
        # Get the biomarker name
        name = biomarker.get("name", "").strip()
        original_name = biomarker.get("original_name", name).strip()
        
        # Get standardized name (check cache first)
        if name.lower() in _process_biomarker._name_cache:
            standardized_name = _process_biomarker._name_cache[name.lower()]
        else:
            standardized_name = get_standardized_biomarker_name(name)
            _process_biomarker._name_cache[name.lower()] = standardized_name
        
        # Use the standardized name if available, otherwise use the original
        result["name"] = standardized_name or name
        result["original_name"] = original_name
        
        # Process value
        try:
            value = biomarker.get("value")
            if value is not None:
                if isinstance(value, str):
                    # Try to convert string value to number
                    value = float(value.replace(",", "").strip())
                result["value"] = float(value)  # Ensure it's a float
            else:
                result["value"] = None
                
            # Keep track of original value
            result["original_value"] = biomarker.get("original_value", str(value))
        except (ValueError, TypeError) as e:
            logger.warning(f"[VALUE_PROCESSING_ERROR] Could not process value {biomarker.get('value')}: {str(e)}")
            result["value"] = None
        
        # Process unit
        unit = biomarker.get("unit", "").strip()
        original_unit = biomarker.get("original_unit", unit).strip()
        
        # Standardize unit
        standard_unit = convert_to_standard_unit(name, unit)
        result["unit"] = standard_unit or unit
        result["original_unit"] = original_unit
        
        # Process reference range
        ref_range = biomarker.get("reference_range", "").strip()
        
        # Try to extract reference range values efficiently using regex
        ref_range_low = result.get("reference_range_low", None)
        ref_range_high = result.get("reference_range_high", None)
        
        if ref_range and not (ref_range_low and ref_range_high):
            import re
            # Try common patterns for reference ranges
            range_patterns = [
                r'(\d+\.?\d*)\s*-\s*(\d+\.?\d*)',  # Standard range: 10-20
                r'(\d+\.?\d*)\s*to\s*(\d+\.?\d*)',  # Range with "to": 10 to 20
                r'<\s*(\d+\.?\d*)',                 # Upper limit: <20
                r'>\s*(\d+\.?\d*)',                 # Lower limit: >10
                r'≤\s*(\d+\.?\d*)',                 # Upper limit: ≤20
                r'≥\s*(\d+\.?\d*)'                  # Lower limit: ≥10
            ]
            
            for pattern in range_patterns:
                match = re.search(pattern, ref_range)
                if match:
                    if len(match.groups()) == 2:
                        # Handle range (e.g., 10-20)
                        try:
                            ref_range_low = float(match.group(1))
                            ref_range_high = float(match.group(2))
                            break
                        except (ValueError, TypeError):
                            pass
                    elif len(match.groups()) == 1:
                        # Handle upper/lower limits (e.g., <20 or >10)
                        try:
                            if pattern.startswith('<') or pattern.startswith('≤'):
                                ref_range_high = float(match.group(1))
                                ref_range_low = None
                            elif pattern.startswith('>') or pattern.startswith('≥'):
                                ref_range_low = float(match.group(1))
                                ref_range_high = None
                            break
                        except (ValueError, TypeError):
                            pass
        
        # Add reference range data to result
        result["reference_range"] = ref_range
        
        if ref_range_low is not None:
            result["reference_range_low"] = float(ref_range_low)
        
        if ref_range_high is not None:
            result["reference_range_high"] = float(ref_range_high)
        
        # Get the biomarker category (if not already present)
        if not biomarker.get("category"):
            result["category"] = get_biomarker_category(name)
        
        # Check if value is abnormal
        is_abnormal = False
        
        # Calculate abnormal status if we have both value and reference range data
        if result.get("value") is not None:
            current_value = result["value"]
            
            if ref_range_low is not None and current_value < ref_range_low:
                is_abnormal = True
            elif ref_range_high is not None and current_value > ref_range_high:
                is_abnormal = True
        
        result["is_abnormal"] = is_abnormal
        
        # Ensure confidence value is a float between 0 and 1
        confidence = biomarker.get("confidence", 0.8)
        try:
            confidence = float(confidence)
            result["confidence"] = max(0.0, min(1.0, confidence))  # Clamp between 0 and 1
        except (ValueError, TypeError):
            result["confidence"] = 0.8  # Default to 0.8 if confidence can't be parsed
        
        return result
    except Exception as e:
        logger.error(f"[BIOMARKER_PROCESSING_ERROR] Error processing biomarker {biomarker.get('name', 'unknown')}: {str(e)}")
        return biomarker  # Return the original if processing fails

def parse_biomarkers_from_text(text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Extract biomarkers from text using pattern matching as a fallback method.
    This is used when Claude API fails or times out.
    
    Args:
        text: Text content from lab report
        
    Returns:
        Tuple of (biomarkers list, metadata dict)
    """
    logger.info("[FALLBACK_PARSER] Starting fallback pattern-based biomarker extraction")
    start_time = datetime.now()
    
    # Preprocess the text to improve pattern matching
    processed_text = _preprocess_text_for_claude(text)
    
    # Patterns for detecting biomarkers
    biomarker_patterns = [
        # Pattern 1: Name: Value Unit (Reference Range)
        r'([A-Za-z][A-Za-z0-9\s,\-\(\)]+)[\s:]+([<>]?\s*\d+\.?\d*)(?:\s*)([a-zA-Z/%µ]+)?(?:\s*[\(\[\{]([^)\]}]+)[\)\]\}])?',
        
        # Pattern 2: Name (Reference Range): Value Unit
        r'([A-Za-z][A-Za-z0-9\s,\-\(\)]+)(?:\s*[\(\[\{]([^)\]}]+)[\)\]\}])?\s*[:=]\s*([<>]?\s*\d+\.?\d*)(?:\s*)([a-zA-Z/%µ]+)?',
        
        # Pattern 3: Name Value Unit
        r'([A-Za-z][A-Za-z0-9\s,\-\(\)]+)(?:\s+)([<>]?\s*\d+\.?\d*)(?:\s+)([a-zA-Z/%µ]+)',
        
        # Pattern 4: Value Unit name
        r'(\d+\.?\d*)\s*([a-zA-Z/%µ]+)\s+([A-Za-z][A-Za-z0-9\s,\-\(\)]+)',
        
        # Pattern 5: Table-like format: Name Value Unit Reference
        r'([A-Za-z][A-Za-z0-9\s,\-\(\)]+)\s+(\d+\.?\d*)\s+([a-zA-Z/%µ]+)\s+(\d+\.?\d*\s*-\s*\d+\.?\d*)'
    ]
    
    # Regular expressions for reference ranges
    reference_range_patterns = [
        r'(\d+\.?\d*)\s*-\s*(\d+\.?\d*)',  # 10-20
        r'(\d+\.?\d*)\s*to\s*(\d+\.?\d*)',  # 10 to 20
        r'<\s*(\d+\.?\d*)',                 # <20
        r'>\s*(\d+\.?\d*)',                 # >10
        r'≤\s*(\d+\.?\d*)',                 # ≤20
        r'≥\s*(\d+\.?\d*)'                  # ≥10
    ]
    
    # Flag patterns to detect abnormal results
    flag_patterns = {
        'high': [r'H', r'HIGH', r'ELEVATED', r'\^', r'↑', r'ABOVE', r'INCREASED'],
        'low': [r'L', r'LOW', r'DECREASED', r'v', r'↓', r'BELOW', r'DEFICIENT'],
        'abnormal': [r'A', r'ABN', r'ABNORMAL', r'\*', r'!']
    }
    
    # Collection for found biomarkers
    biomarkers = []
    used_names = set()  # To avoid duplicates
    
    # Metadata extraction
    metadata = {}
    
    # Extract metadata from common patterns
    metadata_patterns = {
        'lab_name': [
            r'(?:Lab(?:oratory)?|Test Center):\s*([A-Za-z0-9\s,\.\-&]+)',
            r'([A-Za-z0-9\s,\.\-&]+)\s+Laboratory',
            r'Report\s+from\s+([A-Za-z0-9\s,\.\-&]+)'
        ],
        'report_date': [
            r'(?:Report|Collection)\s+Date:\s*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
            r'Date\s+of\s+Report:\s*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
            r'Date:\s*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
            r'(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})'
        ],
        'provider': [
            r'(?:Doctor|Physician|Provider|Ordering Provider):\s*([^,\n\r]+)',
            r'(?:MD|DO):\s*([^,\n\r]+)',
            r'Dr\.\s+([^,\n\r]+)'
        ]
    }
    
    # Extract metadata from text
    for key, patterns in metadata_patterns.items():
        for pattern in patterns:
            match = re.search(pattern, processed_text, re.IGNORECASE)
            if match:
                metadata[key] = match.group(1).strip()
                break
                
    # Redact any potential patient identifiers
    metadata['patient_name'] = "REDACTED"
    
    # Find biomarkers using each pattern
    logger.debug("[FALLBACK_PARSER] Searching for biomarkers using pattern matching")
    lines = processed_text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or len(line) < 5:  # Skip empty or very short lines
            continue
            
        # Try each pattern on the current line
        biomarker_found = False
        
        for pattern in biomarker_patterns:
            matches = re.finditer(pattern, line, re.IGNORECASE)
            
            for match in matches:
                try:
                    groups = match.groups()
                    
                    # Skip if we don't have enough information
                    if not groups or len(groups) < 2:
                        continue
                        
                    if len(groups) >= 4:  # Pattern 1 or 2
                        if re.search(r'^\d', groups[0]) or len(groups[0]) < 2:
                            # Skip if name starts with a number or is too short
                            continue
                            
                        # Different patterns have different group orders
                        if re.search(r'^\d', groups[2]):  # Pattern 2
                            name = groups[0].strip()
                            value_str = groups[2].strip()
                            unit = groups[3].strip() if len(groups) > 3 and groups[3] else ""
                            ref_range = groups[1].strip() if groups[1] else ""
                        else:  # Pattern 1
                            name = groups[0].strip()
                            value_str = groups[1].strip()
                            unit = groups[2].strip() if groups[2] else ""
                            ref_range = groups[3].strip() if len(groups) > 3 and groups[3] else ""
                    elif len(groups) == 3:  # Pattern 3
                        name = groups[0].strip()
                        value_str = groups[1].strip()
                        unit = groups[2].strip()
                        ref_range = ""
                        
                        # Skip common false positives
                        if re.search(r'page|date|time|name|address|phone|fax|sample|specimen|test|visit|account|id',
                                  name, re.IGNORECASE):
                            continue
                    
                    # Clean up the name and skip if it's still not valid
                    name = name.strip(' :,;-_').rstrip(' :,;-_')
                    if len(name) < 2 or not re.match(r'^[A-Za-z]', name):
                        continue
                        
                    # Skip if the name is already used (avoid duplicates)
                    name_key = name.lower()
                    if name_key in used_names:
                        continue
                        
                    # Process the value, handling special cases
                    try:
                        # Handle special values
                        if '<' in value_str:
                            value = float(re.sub(r'[^0-9\.]', '', value_str))
                            flag = 'L'  # Mark as low
                        elif '>' in value_str:
                            value = float(re.sub(r'[^0-9\.]', '', value_str))
                            flag = 'H'  # Mark as high
                        else:
                            value = float(re.sub(r'[^0-9\.]', '', value_str))
                            flag = ''
                    except (ValueError, TypeError):
                        # Skip if we can't parse the value
                        continue
                        
                    # If we got here, we have a valid biomarker
                    biomarker_data = {
                        "name": name,
                        "original_name": name,
                        "value": value,
                        "original_value": value_str,
                        "unit": unit,
                        "original_unit": unit,
                        "reference_range": ref_range,
                        "reference_range_low": None,
                        "reference_range_high": None,
                        "category": "Other",  # Will be standardized later
                        "is_abnormal": False,  # Will be determined later
                        "confidence": 0.7  # Default confidence for pattern matching
                    }
                    
                    # Parse reference range if provided
                    if ref_range:
                        for range_pattern in reference_range_patterns:
                            range_match = re.search(range_pattern, ref_range)
                            if range_match:
                                if len(range_match.groups()) == 2:
                                    # Standard range (e.g., 10-20)
                                    biomarker_data["reference_range_low"] = float(range_match.group(1))
                                    biomarker_data["reference_range_high"] = float(range_match.group(2))
                                    break
                                elif len(range_match.groups()) == 1:
                                    # Upper/lower limit (e.g., <20 or >10)
                                    if range_pattern.startswith('<') or range_pattern.startswith('≤'):
                                        biomarker_data["reference_range_high"] = float(range_match.group(1))
                                    elif range_pattern.startswith('>') or range_pattern.startswith('≥'):
                                        biomarker_data["reference_range_low"] = float(range_match.group(1))
                                    break
                    
                    # Check for flag indicators in the line
                    for flag_type, patterns in flag_patterns.items():
                        for flag_pattern in patterns:
                            if re.search(flag_pattern, line, re.IGNORECASE):
                                if flag_type == 'high' or flag_type == 'low' or flag_type == 'abnormal':
                                    biomarker_data["is_abnormal"] = True
                                    break
                    
                    # Determine abnormal status based on reference range
                    if not biomarker_data["is_abnormal"]:  # Only check if not already set
                        if (biomarker_data["reference_range_low"] is not None and 
                            biomarker_data["value"] < biomarker_data["reference_range_low"]):
                            biomarker_data["is_abnormal"] = True
                        elif (biomarker_data["reference_range_high"] is not None and 
                              biomarker_data["value"] > biomarker_data["reference_range_high"]):
                            biomarker_data["is_abnormal"] = True
                    
                    # Standardize the biomarker data
                    standardized_name = get_standardized_biomarker_name(name)
                    if standardized_name:
                        biomarker_data["name"] = standardized_name
                        
                    # Get category
                    biomarker_data["category"] = get_biomarker_category(biomarker_data["name"])
                    
                    # Try to standardize the unit
                    standard_unit = convert_to_standard_unit(biomarker_data["name"], unit)
                    if standard_unit:
                        biomarker_data["unit"] = standard_unit
                    
                    # Add the biomarker to our results
                    biomarkers.append(biomarker_data)
                    used_names.add(name_key)
                    biomarker_found = True
                    
                except Exception as e:
                    logger.error(f"[PATTERN_MATCH_ERROR] Error processing potential biomarker: {str(e)}")
                    continue
                    
            if biomarker_found:
                break
    
    # Filter out likely false positives
    filtered_biomarkers = []
    for biomarker in biomarkers:
        name = biomarker["name"].lower()
        
        # Skip biomarkers with names that are likely to be false positives
        if (len(name) < 3 or
            re.search(r'page|date|time|sample|test|visit|id|total|final|result|range|normal|low|high|value|unit|level|ratio|reference|index|male|female',
                   name, re.IGNORECASE) and len(name) < 6):
            continue
            
        # Check against the biomarker aliases to improve confidence
        is_known_biomarker = False
        for known_biomarker, aliases in BIOMARKER_ALIASES.items():
            if name == known_biomarker or name in aliases:
                is_known_biomarker = True
                biomarker["confidence"] = 0.85  # Higher confidence for known biomarkers
                break
                
        # Keep biomarkers that pass our filters
        filtered_biomarkers.append(biomarker)
    
    # Log the results
    duration = (datetime.now() - start_time).total_seconds()
    logger.info(f"[FALLBACK_PARSER_COMPLETE] Extracted {len(filtered_biomarkers)} biomarkers in {duration:.2f} seconds")
    
    return filtered_biomarkers, metadata

def categorize_biomarker(name: str) -> str:
    """
    Categorize a biomarker based on its name.
    
            # Save the text content for easier debugging
            debug_fallback_text_path = os.path.join(log_dir, f"fallback_text_response_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
            try:
                with open(debug_fallback_text_path, "w") as f:
                    f.write(text_content)
                logger.debug(f"[FALLBACK_TEXT_SAVED] Fallback text response saved to {debug_fallback_text_path}")
            except Exception as e:
                logger.error(f"[FALLBACK_TEXT_SAVE_ERROR] Could not save fallback text: {str(e)}")
            
            # Log details about the response including the full response
            logger.debug(f"[FALLBACK_RESPONSE_LENGTH] Fallback response length: {len(text_content)} chars")
            logger.debug(f"[FALLBACK_RESPONSE_PREVIEW] First 500 chars: {text_content[:500]}...")
            logger.info(f"[FALLBACK_FULL_RESPONSE] FULL FALLBACK RESPONSE: {text_content}")
            
            # Extract JSON using regex
            json_pattern = r'(\{.*\})'
            json_match = re.search(json_pattern, text_content, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(0)
                
                # Save the extracted JSON for debugging
                debug_fallback_json_path = os.path.join(log_dir, f"fallback_extracted_json_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                try:
                    with open(debug_fallback_json_path, "w") as f:
                        f.write(json_str)
                    logger.debug(f"[FALLBACK_JSON_SAVED] Extracted fallback JSON saved to {debug_fallback_json_path}")
                except Exception as e:
                    logger.error(f"[FALLBACK_JSON_SAVE_ERROR] Could not save fallback JSON: {str(e)}")
                
                # Try to repair and load the JSON
                try:
                    repaired_json = _repair_json(json_str)
                    
                    # Save the repaired JSON for debugging
                    debug_fallback_repaired_path = os.path.join(log_dir, f"fallback_repaired_json_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                    try:
                        with open(debug_fallback_repaired_path, "w") as f:
                            f.write(repaired_json)
                        logger.debug(f"[FALLBACK_REPAIRED_JSON_SAVED] Repaired fallback JSON saved to {debug_fallback_repaired_path}")
                    except Exception as e:
                        logger.error(f"[FALLBACK_REPAIRED_SAVE_ERROR] Could not save repaired fallback JSON: {str(e)}")
                    
                    extraction_data = json.loads(repaired_json)
                    
                    # Process the biomarkers
                    biomarkers_simple = extraction_data.get("biomarkers", [])
                    
                    # Convert to our standard format
                    processed_biomarkers = []
                    for biomarker in biomarkers_simple:
                        # Create a more complete biomarker entry
                        processed_biomarker = {
                            "name": biomarker.get("name", "Unknown"),
                            "original_name": biomarker.get("name", "Unknown"),
                            "value": float(biomarker.get("value", 0)),
                            "original_value": str(biomarker.get("value", "")),
                            "unit": biomarker.get("unit", ""),
                            "original_unit": biomarker.get("unit", ""),
                            "reference_range_text": biomarker.get("reference_range", ""),
                            "category": "Other",
                            "is_abnormal": False,
                            "confidence": 0.7  # Lower confidence for fallback method
                        }
                        
                        # Try to parse reference range
                        ref_range = biomarker.get("reference_range", "")
                        if ref_range:
                            ref_low, ref_high, _ = parse_reference_range(ref_range)
                            processed_biomarker["reference_range_low"] = ref_low
                            processed_biomarker["reference_range_high"] = ref_high
                        
                        processed_biomarkers.append(processed_biomarker)
                    
                    logger.info(f"[FALLBACK_API_EXTRACTION] Extracted {len(processed_biomarkers)} biomarkers with fallback method")
                    
                    # Save the processed biomarkers for debugging
                    debug_processed_biomarkers_path = os.path.join(log_dir, f"fallback_processed_biomarkers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                    try:
                        with open(debug_processed_biomarkers_path, "w") as f:
                            json.dump(processed_biomarkers, f, indent=2)
                        logger.debug(f"[FALLBACK_PROCESSED_SAVED] Processed biomarkers saved to {debug_processed_biomarkers_path}")
                    except Exception as e:
                        logger.error(f"[FALLBACK_PROCESSED_SAVE_ERROR] Could not save processed biomarkers: {str(e)}")
                    
                    return processed_biomarkers, extraction_data.get("metadata", {})
                    
                except json.JSONDecodeError as e:
                    logger.error(f"[FALLBACK_JSON_ERROR] Error parsing fallback JSON: {str(e)}")
                    logger.error(f"[FALLBACK_JSON_CONTENT] Fallback JSON content: {json_str[:500]}...")
            else:
                logger.error("[FALLBACK_JSON_MISSING] No JSON found in fallback response")
                logger.error(f"[FALLBACK_RESPONSE_FULL] Full fallback response: {text_content}")
        else:
            logger.error(f"[FALLBACK_API_ERROR] Fallback API request failed with status {response.status_code}")
    
    except Exception as e:
        logger.error(f"[FALLBACK_API_EXCEPTION] Exception during fallback API call: {str(e)}")
    
    # If all else fails, use the text-based parser
    logger.warning("[FALLBACK_TO_TEXT_PARSER] All Claude API methods failed. Using fallback text parser.")
    fallback_results = parse_biomarkers_from_text(text)
    logger.info(f"[TEXT_PARSER] Text parser found {len(fallback_results)} biomarkers")
    return fallback_results, {} 

def _process_large_text(text: str, filename: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Process large text by breaking it into chunks and extracting biomarkers from each chunk.
    
    Args:
        text: The preprocessed text content
        filename: Name of the file for logging
        
    Returns:
        Tuple containing a list of biomarkers and metadata
    """
    logger.info(f"[LARGE_TEXT_PROCESSING] Breaking text into chunks for processing ({len(text)} chars)")
    chunks = chunk_text(text)
    
    all_biomarkers = []
    all_metadata = {}
    
    # Process each chunk separately
    for i, chunk in enumerate(chunks):
        logger.info(f"[CHUNK_PROCESSING] Processing chunk {i+1}/{len(chunks)} ({len(chunk)} chars)")
        
        # Create a modified filename for logging
        chunk_filename = f"{filename} (chunk {i+1}/{len(chunks)})"
        
        # Set a different prompt for chunks to clarify these are partial extractions
        # Extract biomarkers from this chunk using a modified call
        try:
            chunk_biomarkers, chunk_metadata = _extract_biomarkers_from_chunk(chunk, chunk_filename)
            
            # Add biomarkers from this chunk to overall list
            all_biomarkers.extend(chunk_biomarkers)
            
            # Merge metadata if this is the first chunk with metadata
            if chunk_metadata and not all_metadata:
                all_metadata = chunk_metadata
                
            logger.info(f"[CHUNK_COMPLETE] Chunk {i+1}/{len(chunks)} yielded {len(chunk_biomarkers)} biomarkers")
            
        except Exception as e:
            logger.error(f"[CHUNK_ERROR] Error processing chunk {i+1}/{len(chunks)}: {str(e)}")
            # Continue with next chunk
    
    # Deduplicate biomarkers
    unique_biomarkers = []
    seen_biomarkers = set()
    for biomarker in all_biomarkers:
        name = biomarker.get('name', '').lower()
        value = str(biomarker.get('value', ''))
        key = (name, value)
        if key not in seen_biomarkers:
            seen_biomarkers.add(key)
            unique_biomarkers.append(biomarker)
    
    logger.info(f"[DEDUPLICATION] Removed {len(all_biomarkers) - len(unique_biomarkers)} duplicate biomarkers")
    
    # If no biomarkers found across all chunks, fall back to text pattern matching
    if not unique_biomarkers:
        logger.warning("[NO_BIOMARKERS_FOUND] No biomarkers found in any chunks")
        logger.info("[FALLBACK_TO_TEXT_PARSER] Using fallback parser")
        return parse_biomarkers_from_text(text)
        
    return unique_biomarkers, all_metadata

def _extract_biomarkers_from_chunk(chunk: str, chunk_filename: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Extract biomarkers from a single text chunk using Claude API.
    Similar to extract_biomarkers_with_claude but tailored for chunks.
    
    Args:
        chunk: Text chunk to process
        chunk_filename: Identifier for logging
        
    Returns:
        Tuple containing list of biomarkers and metadata
    """
    # Create a smaller, focused prompt for the chunk
    prompt = f"""
You are extracting biomarkers from a PARTIAL section of a lab report. Focus ONLY on the biomarkers in this section.

Extract all measurable biomarkers with:
- name and original_name
- value (as a number) and original_value (as text)
- unit and original_unit
- reference range text and numeric bounds when available
- category and abnormal flag

Respond with ONLY valid JSON in this format:
{{
  "biomarkers": [
    {{
      "name": "Glucose",
      "original_name": "Glucose, Fasting",
      "value": 95,
      "original_value": "95",
      "unit": "mg/dL",
      "original_unit": "mg/dL",
      "reference_range": "70-99 mg/dL",
      "reference_range_low": 70,
      "reference_range_high": 99,
      "category": "Metabolic",
      "is_abnormal": false,
      "confidence": 0.98
    }},
    ...
  ],
  "metadata": {{
    "lab_name": "LabCorp",
    "report_date": "2022-04-15"
  }}
}}

Here is the text section:

{chunk}
"""
    
    try:
        logger.debug(f"[CHUNK_API_CALL] Sending chunk request to Claude API ({len(chunk)} chars)")
        api_start_time = datetime.now()
        
        # Get the API key
        api_key = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY")
        if not api_key:
            raise ValueError("Claude API key not found")
        
        import anthropic
        client = anthropic.Anthropic(api_key=api_key)
        
        # Use a faster model with timeout
        @with_timeout(timeout_seconds=MAX_API_TIMEOUT, default_return=None)
        def call_claude_api_for_chunk():
            response = client.messages.create(
                model="claude-3-haiku-20240307",  # Use faster model for chunks
                max_tokens=4000,
                temperature=0.0,
                system="Extract biomarkers from this lab report section. Output ONLY valid JSON.",
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response
        
        # Call API with retry mechanism
        response = None
        for attempt in range(MAX_RETRIES + 1):
            response = call_claude_api_for_chunk()
            if response is not None:
                break
                
            if attempt < MAX_RETRIES:
                backoff_time = min(20, (2 ** attempt) + random.uniform(0, 1))
                logger.warning(f"[CHUNK_API_RETRY] Attempt {attempt+1}/{MAX_RETRIES+1} timed out. Retrying in {backoff_time:.2f} seconds...")
                time.sleep(backoff_time)
        
        if response is None:
            logger.error("[CHUNK_API_TIMEOUT] Chunk API call timed out after all retries")
            return [], {}
        
        # Process response
        api_duration = (datetime.now() - api_start_time).total_seconds()
        logger.info(f"[CHUNK_API_RESPONSE] Received chunk response in {api_duration:.2f} seconds")
        
        response_content = response.content[0].text
        
        # Try to parse the JSON response using the same logic as main function
        # Extract the JSON part from the response using regex
        import re
        json_match = re.search(r'({[\s\S]*})', response_content)
        
        if json_match:
            json_str = json_match.group(1)
            
            # Try to fix malformed JSON if needed
            try:
                parsed_response = json.loads(json_str)
            except json.JSONDecodeError:
                json_str = _repair_json(json_str)
                parsed_response = json.loads(json_str)
            
            biomarkers = parsed_response.get("biomarkers", [])
            metadata = parsed_response.get("metadata", {})
            
            # Process biomarkers (simplified for chunks)
            processed_biomarkers = []
            for biomarker in biomarkers:
                # Skip entries with low confidence
                if float(biomarker.get("confidence", 0.0)) < 0.6:
                    continue
                
                try:
                    processed_biomarker = _process_biomarker(biomarker)
                    processed_biomarkers.append(processed_biomarker)
                except Exception as e:
                    logger.error(f"[CHUNK_BIOMARKER_ERROR] Error processing biomarker: {str(e)}")
            
            return processed_biomarkers, metadata
        else:
            logger.error("[CHUNK_JSON_ERROR] Could not extract JSON from chunk response")
            return [], {}
            
    except Exception as e:
        logger.error(f"[CHUNK_PROCESSING_ERROR] Error processing chunk: {str(e)}")
        return [], {} 

def extract_biomarkers(text: str, filename: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Main entry point for biomarker extraction from lab reports.
    Attempts to extract biomarkers first with Claude API, falling back to pattern matching if needed.
    
    Args:
        text: Text content from PDF
        filename: Name of the file for logging
        
    Returns:
        Tuple containing list of biomarkers and metadata
    """
    start_time = datetime.now()
    logger.info(f"[BIOMARKER_EXTRACTION_START] Starting biomarker extraction for {filename}")
    
    try:
        # First attempt: Use Claude API for extraction
        biomarkers, metadata = extract_biomarkers_with_claude(text, filename)
        
        # If we got biomarkers, validate and return them
        if biomarkers:
            # Log performance metrics
            extraction_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"[BIOMARKER_EXTRACTION_SUCCESS] Extracted {len(biomarkers)} biomarkers in {extraction_time:.2f} seconds")
            
            # Save the extraction results if debugging is enabled
            if SAVE_CLAUDE_RESPONSES:
                try:
                    results_path = os.path.join(log_dir, f"extraction_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                    with open(results_path, "w") as f:
                        json.dump({
                            "biomarkers": biomarkers,
                            "metadata": metadata,
                            "extraction_time_seconds": extraction_time,
                            "source_file": filename
                        }, f, indent=2)
                    logger.debug(f"[EXTRACTION_RESULTS_SAVED] Saved extraction results to {results_path}")
                except Exception as e:
                    logger.warning(f"[EXTRACTION_RESULTS_SAVE_ERROR] Could not save extraction results: {str(e)}")
            
            return biomarkers, metadata
    except Exception as e:
        logger.error(f"[BIOMARKER_EXTRACTION_ERROR] Error during extraction: {str(e)}")
    
    # If we reached here, Claude API extraction failed or returned no biomarkers
    # Fall back to pattern matching
    logger.info(f"[BIOMARKER_EXTRACTION_FALLBACK] Using fallback extraction for {filename}")
    biomarkers, metadata = parse_biomarkers_from_text(text)
    
    # Log performance metrics for fallback method
    extraction_time = (datetime.now() - start_time).total_seconds()
    logger.info(f"[BIOMARKER_EXTRACTION_FALLBACK_COMPLETE] Extracted {len(biomarkers)} biomarkers with fallback method in {extraction_time:.2f} seconds")
    
    return biomarkers, metadata