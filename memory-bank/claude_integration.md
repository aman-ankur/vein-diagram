# Vein Diagram: Claude API Integration

This document outlines how the Vein Diagram application integrates with the Anthropic Claude API to enhance functionality.

## Purpose

The Claude API is leveraged for four primary tasks:

1.  **Biomarker Explanations**: Generating user-friendly explanations for specific biomarker results, including general information about the biomarker and context based on the user's specific value and reference range. (Handled by `app/services/llm_service.py`)
2.  **Metadata Extraction**: Extracting structured patient and report metadata (like name, DOB, gender, patient ID, lab name, report date) from the unstructured text content of the ***initial pages*** of uploaded PDF lab reports. This aids in profile matching and data organization. (Handled by `app/services/metadata_parser.py`)
3.  **Biomarker Extraction**: Extracting structured biomarker data (name, value, unit, range, etc.) from the text of ***relevant, filtered pages*** of uploaded PDF lab reports, processed sequentially. (Handled by `app/services/biomarker_parser.py`)
4.  **Health Summary Generation**: Generating a structured, personalized health summary based on profile and biomarker history. (Handled by `app/services/health_summary_service.py`)

## Configuration

-   **API Key**: The integration requires an Anthropic API key, which must be set in the environment variable `ANTHROPIC_API_KEY` (or `CLAUDE_API_KEY` as a fallback). If the key is not found, the services will use mock/fallback responses.
-   **API Endpoint**: `https://api.anthropic.com/v1/messages`
-   **Anthropic Version Header**: `2023-06-01`

## Models Used

-   **Biomarker Explanations**: `claude-3-haiku-20240307` (Chosen for speed and cost-effectiveness for explanation tasks).
-   **Metadata Extraction**: `claude-3-5-sonnet-20241022` (Updated from deprecated `claude-3-sonnet-20240229` for improved accuracy and future compatibility).
-   **Biomarker Extraction (Sequential per page)**: `claude-3-5-sonnet-20241022` (Updated from deprecated model for enhanced accuracy in structured biomarker data extraction).

## Content Optimization & Token Management

### Token Optimization Pipeline
- **Function**: `content_optimization.compress_text_content`
- **Purpose**: Aggressive content compression to reduce Claude API token usage by 99%+ while preserving biomarker data
- **Techniques**:
  - Removes administrative data (contact info, CIN numbers, fax numbers, email addresses)
  - Eliminates boilerplate text and method descriptions
  - Compresses repeated headers and formatting
  - Standardizes number formats and removes geographic references
  - Filters out document structure elements (headers, footers, page numbers)
- **Integration**: Applied before all Claude API calls for metadata and biomarker extraction
- **Results**: Achieved 99%+ token reduction (e.g., 726 â†’ 5 tokens) while maintaining biomarker information integrity

## Enhanced Prompting Strategy

### 1. Biomarker Explanations (`llm_service.py`)

-   **Goal**: Generate both a general overview and a personalized interpretation.
-   **Input**: Biomarker name, value, unit, reference range, status (normal/abnormal).
-   **Structure**:
    -   System Prompt: Instructs Claude to act as a helpful medical assistant explaining results in plain language.
    -   User Prompt: Provides the specific biomarker details and requests explanations formatted into `ABOUT_THIS_BIOMARKER:` and `YOUR_RESULTS:` sections, including a medical disclaimer.
-   **Parsing**: The service attempts to parse the response based on the section headers. If parsing fails, it uses fallback logic to split the text.

### 2. Enhanced Metadata Extraction (`metadata_parser.py`)

-   **Goal**: Extract specific, predefined metadata fields only.
-   **Input**: Optimized/compressed text content from the *initial pages* of a PDF.
-   **Structure**:
    -   System Prompt: Instructs Claude to act as a metadata extraction expert, focusing *only* on specific fields (lab_name, report_date, patient_name, patient_dob, patient_gender, patient_id, patient_age) and outputting *only* valid JSON in a predefined format (`{"metadata": {...}}`). Explicitly tells Claude to ignore biomarker results.
    -   User Prompt: Provides the optimized text fragment from the *initial pages*.
-   **Parsing**: The service uses regex to find the JSON block in the response and `json.loads` to parse it. Includes logic (`_repair_json`) to attempt fixing minor JSON formatting issues before parsing.

### 3. Enhanced Biomarker Extraction (`biomarker_parser.py`)

-   **Goal**: Extract specific biomarker fields from individual relevant pages with strict validation.
-   **Input**: Optimized/compressed text content from a *single, filtered* PDF page.
-   **Enhanced Structure**:
    -   **System Prompt**: Comprehensive instructions defining valid biomarkers as requiring:
      - Recognizable medical test name
      - Numerical value with appropriate unit
      - Biological sample measurement
    -   **Validation Rules**: Explicit "DO NOT EXTRACT" list including:
      - Contact information (phone, fax, email, addresses)
      - Administrative data (CIN numbers, registration codes)
      - Document headers and footers
      - Method descriptions and technical procedures
      - Qualitative results without measurements ("Normal", "High", "Low")
      - Geographic information and lab locations
    -   **Examples**: Clear valid vs invalid extraction examples
    -   **Output Format**: Strict JSON format (`{"biomarkers": [...]}`) with specific fields
-   **Enhanced Parsing**: 
    - Advanced JSON repair logic handling common Claude API response errors
    - Specific fixes for truncated JSON and malformed responses
    - Comprehensive fallback to improved regex parser with 100+ invalid pattern filters
-   **Sequential Processing**: This extraction is called sequentially for each relevant page identified by the filtering process in `pdf_service.py`.

### 4. Health Summary Generation (`health_summary_service.py`)

- **Goal**: Generate a structured, personalized health summary based on profile and biomarker history.
- **Input**: User profile details (name, age, gender, favorites), formatted biomarker history (recent values, trends, full history).
- **Structure**:
    - System Prompt: Instructs Claude to act as an empathetic health monitoring assistant.
    - User Prompt: Provides user info, biomarker data, and strict formatting rules.
    - **Format Requirements**: Emphasizes exact output structure:
        - Start each section *directly* with its emoji (ðŸ’¡, ðŸ“ˆ, ðŸ‘€) on a new line. NO text headers (e.g., "KEY INSIGHTS:") allowed after the emoji.
        - Each point *within* a section starts on a new line with "â€¢ " (bullet and space).
        - Strict adherence to `EMOJI\nâ€¢ Point\nâ€¢ Point\nEMOJI\nâ€¢ Point...` format.
        - No greetings or conclusions.
- **Parsing (Frontend)**: The frontend (`VisualizationPage.tsx`) is responsible for parsing this string based on the emoji delimiters and bullet points to render the structured UI components.

## Enhanced Biomarker Validation & Filtering

### Comprehensive Invalid Pattern Detection
The system now includes extensive filtering to prevent extraction of non-biomarker data:

- **Administrative Data**: Fax numbers, CIN codes, email addresses, phone numbers, registration numbers
- **Geographic Information**: City names, addresses, postal codes, state names
- **Document Structure**: Headers, footers, page numbers, method descriptions, technical procedures
- **Qualitative Results**: "Normal", "Abnormal", "High", "Low" without numerical measurements
- **Contact Information**: Lab contact details, customer service information
- **Time/Date References**: Standalone dates, time stamps without biomarker context
- **Common Words**: Generic terms that might appear in medical documents but aren't biomarkers

### Enhanced Categorization
- Added "Immunology" category for IgE, antibodies, and immune system markers
- Improved classification accuracy for specialized biomarkers

## Caching (Biomarker Explanations Only)

-   An in-memory cache (`ExplanationCache` in `llm_service.py`) is used to store generated explanations and reduce redundant API calls.
    -   **General Explanations**: Cached per biomarker name. TTL: 30 days.
    -   **Specific Explanations**: Cached per unique combination of biomarker name, value, and status. TTL: 7 days.
-   *Note: This is a simple in-memory cache suitable for development. A persistent cache like Redis would be needed for production.*

## Enhanced Error Handling & Fallbacks

-   **API Key Missing**: Logs a warning and returns mock/placeholder responses.
-   **API Errors (HTTP Status != 200)**: Logs the error and returns generic "unavailable" messages.
-   **Timeout**:
    -   Metadata extraction uses a `with_timeout` decorator (45 seconds) and returns an empty dictionary on timeout.
    -   Biomarker explanation uses `httpx` client timeout (30 seconds) and returns "unavailable" messages on timeout.
    -   Biomarker extraction (per page) uses a `with_timeout` decorator (600 seconds / 10 minutes *per page*) and falls back to the enhanced regex parser (`parse_biomarkers_from_text`) for that specific page on timeout.
-   **Request Errors (`httpx.RequestError`)**: Logs the error and returns "unavailable" messages or an empty dictionary/list.
-   **Enhanced Response Parsing Errors**:
    -   Explanation: Logs the error and attempts fallback splitting or returns generic messages.
    -   Metadata: Logs the error, attempts JSON repair, and returns an empty dictionary if parsing/repair fails.
    -   Biomarker Extraction (per page): Logs the error, attempts extensive JSON repair with specific fixes for common Claude API errors, and falls back to the enhanced regex parser for that specific page if parsing/repair fails.
-   **Unexpected Errors**: Catches general exceptions, logs them, and returns generic error messages/empty dictionaries/lists.
-   **Enhanced Fallback Parser (Biomarker Extraction)**: If Claude API calls fail for biomarker extraction on a specific page (timeout, errors, invalid JSON), the system falls back to a significantly improved regex-based parser (`parse_biomarkers_from_text`) with comprehensive invalid pattern filtering for *that page only*.

## Recent Critical Improvements (May 2025)

### Token Optimization Breakthrough
- **Problem**: Content optimization was increasing tokens by 106% instead of reducing them
- **Solution**: Implemented aggressive compression techniques in `content_optimization.py`
- **Result**: Achieved 99%+ token reduction while preserving biomarker information
- **Impact**: Dramatically reduced API costs and improved processing speed

### Enhanced Prompt Engineering
- **Problem**: Prompts were not specific enough about valid biomarker definitions
- **Solution**: Comprehensive prompts with explicit valid/invalid examples and strict validation rules
- **Result**: Eliminated extraction of contact info, administrative data, and qualitative results
- **Impact**: Significantly improved extraction accuracy and reduced false positives

### Model Updates
- **Problem**: Using deprecated Claude models (`claude-3-sonnet-20240229`)
- **Solution**: Updated to latest models (`claude-3-5-sonnet-20241022`)
- **Result**: Improved extraction accuracy and future-proofed API calls
- **Impact**: Better performance and continued API compatibility

### Advanced Error Handling
- **Problem**: JSON parsing failures and truncated responses causing extraction failures
- **Solution**: Enhanced JSON repair logic with specific fixes for common Claude API response patterns
- **Result**: Improved robustness and reduced fallback to regex parser
- **Impact**: Higher success rate for LLM-based extraction

## Key Considerations

-   **Cost**: API calls incur costs. **Token optimization achieved 99%+ reduction**, dramatically lowering costs. Sequential processing for biomarker extraction might increase the number of calls compared to a single large call, but reduces token usage per call and avoids wasted processing on irrelevant pages. Caching (for explanations) and using faster models like Haiku help mitigate costs.
-   **Latency**: API calls add latency. Metadata extraction is done early. Sequential biomarker extraction processes pages one by one, potentially increasing overall wall-clock time for biomarker extraction compared to a single (potentially failing) large call, but provides incremental progress and avoids complete failure due to timeouts on large inputs. **Token optimization significantly reduces processing time per call**.
-   **Accuracy**: Extraction accuracy significantly improved through enhanced prompts, comprehensive filtering, and model updates. The page filtering step might occasionally miss relevant pages or include irrelevant ones. **Enhanced fallback mechanisms (improved regex parser) have much better accuracy than before**.
-   **Rate Limits**: Production usage may require handling API rate limits, especially with sequential calls.
-   **Prompt Engineering**: The effectiveness relies on carefully crafted prompts instructing the model on the desired output format and content for each task (metadata, biomarkers per page, explanations). **Recent improvements show dramatic accuracy gains through better prompt engineering**.

## Testing & Validation

### Token Optimization Testing
- Created `test_token_optimization.py` to validate improvements
- Confirmed 99%+ token reduction while preserving biomarker data
- Verified removal of all problematic patterns (Fax, CIN, Email, etc.)

### Biomarker Filtering Testing
- Comprehensive testing of invalid pattern detection
- Validation of enhanced fallback parser performance
- Confirmation of improved categorization accuracy
